\subsection*{Social Learning in Networks}

We adapt the model in Brandl (2024). Specifically, we let $N = \{1, \ldots, n\}$ be the set of agents, and $T = \{1, 2, \ldots, t \}$ be the finite set of periods. In each period, each agent chooses an action $a$ from the same finite set of actions $A$. The finite set of states of the world is $\Omega$, where the true state $\omega \in \Omega$ is a random variable with full support distribution over $\Omega$. Agents share the same utility function $u : A \times \Omega \to \mathbb{R}$ that depends on their own action and the state of the world, where $a_\theta$ denotes the unique optimal (correct) action in that state. It's also assumed that $a_\theta \neq a_{\theta'}$ for any two distinct states $\theta, \theta' \in \Omega$. In each period $t \in T$, each agent $i$ privately observes a signal $s_t^i$ from a set of signals $S$. Conditional on each state $\theta$, $s_t^i$ has distribution $\mu_\theta \in \Delta(S)$, and signals are independent across agents and periods. Observing the signal realization $s \in S$ changes the log-likelihood ratio of the observing agent between the states $\theta$ and $\theta'$ by,
\[
\ell_{\theta, \theta'}(s) = \log \frac{d\mu_\theta(s)}{d\mu_{\theta'}(s)}.
\]
Each agent $i$ also observes the actions of her neighbors $N^i \subset N$, thus the information available to Agent $i$ in period $t$ before choosing an action is the collection of information sets $\mathcal{I}^i_{\leq t} = S^t \times A^{|N^i| \times (t-1)}$. It's said that Agent $i$ makes a mistake in period $t$ if $a_t^i \neq a_\omega$, and that $i$ learns at rate $r$ if,
\[
r = \liminf_{t \to \infty} -\frac{1}{t} \log P(a_t^i \neq a_\omega).
\]
Theorem 1 states that for any number of agents and any strategy profile, there exists an agent $i$ such that,
\[r^i \leq r_{bdd} = \min_{\theta \neq \theta'}\{E_\theta[\ell_{\theta,\theta'}] + E_{\theta'}[\ell_{\theta',\theta}]\}.\]


\subsection*{Partially Observable Markov Games}
Following the MARL literature, we model the social learning framework as a partially observable Markov game defined by the tuple,
\[(N, \Psi, A, \{O_i\}_{i \in N}, \Gamma, R, \gamma)\]
where,
\begin{itemize}
\item $N$ is the set of agents,
\item $\Psi $ is the Markov state space, with individual element $\psi$,
\item $A$ is the shared action space for all agents, with individual element $a$,
\item $O_i$ is the observation space for agent $i$, with individual element $o_i$,
\item $\Gamma : \Psi \times A^n \times \Psi \to [0,1]$ is the transition function,
\item $R: A \times \Psi \to \mathbb{R}$ is the shared reward function for all agents,
\item $\gamma \in [0,1)$ is the discount factor.
\end{itemize}

\subsubsection*{Markov States, Signals \& Actions}
For simplicity, we let $\Omega = \{\theta_1, \ldots, \theta_k\}$ be the set of possible states of the world; $S = \{s_k : \theta_k \in \Omega\}$ be the signal space, with $s_k$ being the signal for state $\theta_k$ and $A = \{a_k : \theta_k \in \Omega\}$ be the action space, with $a_k$ being the optimal action for state $\theta_k$. We can then define the Markov state space as 
\begin{equation*}
    \Psi = \{\omega\} \times S \times A^n
\end{equation*}
which accounts for the true state of the world, current period signal and current period actions. We leave the tracking of signal and action histories to the Recurrent Neural Networks (RNNs), thereby avoiding exponentially increasing Markov state space formulations.

\subsubsection*{Observations}
The observation for each agent $i$ at time $t$ is,
\begin{equation*}
o^i_t = (s^i_t, \bm{a_t}) \in O_i,
\end{equation*}
where $O_i = S \times A^n$ and $\bm{a_t} = \{a_t^1, \ldots, a_t^n\}$ is the action profile at period $t$.

\subsubsection*{Transitions}
The stochastic components of the Markov state transitions are the signal realizations and the actions taken by the agents. Formally, for state 
$\psi_t = (\omega, (s^i_t)_{i \in N}, \bm{a_t})$ and next state ${}\psi_{t+1} = (\omega, (s^i_{t+1})_{i \in N}, \bm{a_{t+1}} )$, the transition function can be written as,
\begin{equation*}
    \Gamma(\psi_{t+1}|\psi_t) = \prod_{i \in N} \mu_{\omega}(s^i_{t+1}) \prod_{i \in N} \sigma_t^i(\mathcal{I}^i_{\leq t})(a^i_{t+1})
\end{equation*}
where $\sigma_t^i$ is the \textit{policy} or \textit{strategy} of agent $i$ at period $t$. There are two important points to consider here. First, in this formulation, we condition the policies on action and signal histories that are not part of the Markov state space. The fact that we can still use this information to compute the policies stems from the technology of RNNs that allows for a memory buffer from past observations, without explicitly storing the observations themselves. Second, diverging from the original paper, we allow for policies to be time-dependent, which corresponds to the learning process of the RL agents.
\subsubsection*{Rewards}
The reward functions are identical across agents with $R_i(a, \psi)=R(a,\psi), \: \forall i \in N$. The true reward is the flow payoff $u(a^i_t, \omega)$, where $u: A \times \Omega \rightarrow \mathbb{R}$ is known but its realizations are not observed. Lacking this observation, we follow Huang et al. (2024) and construct an observed payoff function $v(a, s)$ with the property that, 
\[\mathbb{E}_{s \sim \mu^{\theta}}[v(a, s)] = u(a, \theta), \quad \forall \theta \in \Omega.\] Setting $u(a, \omega) = \mathbf{1}_{\{a=a^{\omega}\}}$ the problem of constructing such a function amounts to solving a system of $|\Omega|$ linear equations for each action.
More precisely, let $\bm{\mu}$ be an $k \times k$ matrix representing the signal distributions, where the entry $\mu^{\theta}(s)$ denotes the probability of observing signal $s$ in state $\theta$. We assume $\mu^{\theta}(s_{\theta}) > \mu^{\theta}(s)$, $\forall s \in S$, $\theta \in \Omega$, which implies that signal distributions are linearly independent across states and $\bm{\mu}$ is invertible. Similarly, for each action $a \in A$, define the \textit{utility vector} $\bm{u}_a \in \mathbb{R}^k$ as,
\begin{equation*}
    \bm{u}_a = \begin{bmatrix} u(a, \theta_1) \\ u(a, \theta_2) \\ \vdots \\ u(a, \theta_k) \end{bmatrix}.
\end{equation*}
The vector of observable reward functions $\bm{v}_a \in \mathbb{R}^n$ for action $a$ is the unique solution to the linear system as in,
\begin{align*}
    \bm{\mu} \bm{v}_a &= \bm{u}_a, \\
    \bm{v}_a &= \bm{\mu}^{-1} \bm{u}_a.
\end{align*}
Hence, the reward $v(a, s_j)$ for choosing action $a$ when observing signal $s_j$ is,
\begin{equation*}
    v(a, s_j) = \sum_{l=1}^k \left( \bm{\mu}^{-1} \right)_{jl} u(a, \theta_l),
\end{equation*}
where $\left( \bm{\mu}^{-1} \right)_{jl}$ denotes the $(j, l)$-th element of $\bm{\mu}^{-1}$.

\paragraph*{Binary Case with $|\Omega|=2$.}

For the case with two states of the world and symmetric signals with accuracy $q > 0.5$ we have,
\begin{equation*}
    \bm{\mu} = \begin{bmatrix} q & 1 - q \\ 1 - q & q \end{bmatrix}, \quad \bm{\mu}^{-1} = \frac{1}{2q - 1} \begin{bmatrix} q & -(1 - q) \\ -(1 - q) & q \end{bmatrix}
\end{equation*}
Hence, for action $a$ and flow utility $u(a, \omega) = \mathbf{1}_{\{a = \omega\}}$, the observed reward function becomes,
\begin{equation*}
    R(a, \psi) = v(a, s) = \frac{q \cdot \mathbf{1}_{\{a = s\}} - (1 - q) \cdot \mathbf{1}_{\{a \neq s\}}}{2q - 1}.
\end{equation*}

\subsection*{Recurrent Neural Networks}
Before proceeding with the RNN architecture and learning framework, we define the key activation functions and operations:

The hyperbolic tangent function $\tanh: \mathbb{R} \to (-1,1)$ is defined as,
\[
    \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}.
\]

The sigmoid function $\text{sigmoid} : \mathbb{R} \to (0,1)$ is defined as,
\[
    \sigma(x) = \frac{1}{1 + e^{-x}}.
\]

The softmax function $\text{softmax}: \mathbb{R}^k \to \Delta^{k-1}$ maps a vector to a probability distribution,
\[
    \text{softmax}(x)_i = \frac{e^{x_i}}{\sum_{j=1}^k e^{x_j}}.
\]

The Hadamard (element-wise) product $\odot: \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}^n$ is defined as,
\[
    (x \odot y)_i = x_i y_i.
\]
Following our partial observability setting, each agent $i$ observes both their private signal $s^i_t$ and the action profile $\mathbf{a}_t = (a^1_t, \ldots, a^n_t)$ at time $t$. The collection of these observations up to time $t$ forms the information set $\mathcal{I}^i_{\leq t} = S^t \times A^{|N_i|Ã—(t-1)}$. Rather than including this growing information set in our Markov state space, we employ Recurrent Neural Networks to maintain a compact representation while preserving the essential historical information.

\subsubsection*{RNN Architecture for Information Processing}
For each agent $i$, we implement an RNN that processes the sequence of observations $o_t^i = (s_t^i, \mathbf{a}_t) \in \mathcal{O}_i$ to produce a hidden state $h_t^i \in \mathbb{R}^d$ that encodes the information set $\mathcal{I}_{\leq t}^i$. The RNN updates its hidden state through a recursive function,
\begin{equation*}
h_t^i = f_{\eta_i}(h_{t-1}^i, o_t^i),
\end{equation*}
where $f_{\eta_i}$ is parameterized by $\eta_i$ and implemented using a Gated Recurrent Unit (GRU) architecture, following \cite{cho2014learning}. The GRU utilizes two primary gates - an update gate and a reset gate - to selectively incorporate new information while preserving relevant historical context:
\begin{align*}
z_t^i &= \text{sigmoid}(W_z^i[h_{t-1}^i, s_t^i, \mathbf{a}_t] + b_z^i) & \text{(update gate)} \\
\rho_t^i &= \text{sigmoid}(W_\rho^i[h_{t-1}^i, s_t^i, \mathbf{a}_t] + b_\rho^i) & \text{(reset gate)} \\
\tilde{h}_t^i &= \tanh(W_h^i[\rho_t^i \odot h_{t-1}^i, s_t^i, \mathbf{a}_t] + b_h^i) & \text{(candidate state)} \\
h_t^i &= (1-z_t^i) \odot h_{t-1}^i + z_t^i \odot \tilde{h}_t^i & \text{(final update)}
\end{align*}
Here, $W_z^i, W_\rho^i, W_h^i$ are weight matrices and $b_z^i, b_\rho^i, b_h^i$ are bias vectors specific to agent $i$.

The update gate $z_t^i$ determines how much of the previous hidden state should be retained versus updated with new information. When $z_t^i$ is close to 1, the network prioritizes the new candidate state $\tilde{h}_t^i$; when close to 0, it maintains more of the previous state $h_{t-1}^i$. This mechanism is particularly important in our social learning context, as it allows agents to appropriately weight new signals against accumulated historical evidence.

The reset gate $\rho_t^i$ controls how much of the previous hidden state should influence the computation of the new candidate state $\tilde{h}_t^i$. When $\rho_t^i$ is close to 0, it effectively "forgets" the previous state, allowing the network to drop irrelevant historical information. This is crucial when the agent needs to adapt to significant changes in the environment or when previous beliefs need to be revised based on new, strong evidence.

The candidate state $\tilde{h}_t^i$ is computed using the reset gate-modulated previous state along with current observations. The hyperbolic tangent activation ensures the candidate state values remain bounded between -1 and 1, promoting stable learning dynamics. The strategy or policy $\sigma_t^i$ of agent $i$ at time $t$ is then derived from this hidden state through a policy network,

\begin{equation*}
\sigma_t^i(\mathcal{I}_{\leq t}^i) = \pi_{\phi_i}(h_t^i),
\end{equation*}
where $\pi^i_{\phi_i}$ is a neural network with parameters $\phi_i$ that maps the hidden state to a distribution over actions,

\begin{equation*}
    \pi^i_{\phi_i}(h^i_t) = \text{softmax}(W^i_{\pi}h^i_t + b^i_{\pi}).
\end{equation*}
This architecture effectively augments our original Markov state space $\Psi = \{\omega\} \times S \times A^n$ with the RNN hidden states,

\begin{equation*}
    \tilde{\Psi} = \{\omega\} \times S \times A^n \times \mathcal{H}^n,
\end{equation*}
where $\mathcal{H} \subset \mathbb{R}^d$ is the space of hidden states. The transition function $\Gamma$ is accordingly modified to include the hidden state updates,

\begin{equation*}
    \tilde{\Gamma}(\tilde{\psi}_{t+1}|\tilde{\psi}_t) = \prod_{i \in N} \mu_\omega(s^i_{t+1}) \prod_{i \in N} \pi^i_{\phi_i}(h^i_t),
\end{equation*}
where $\tilde{\psi}_t = (\omega, (s^i_t)_{i \in N}, \mathbf{a}_t, (h^i_t)_{i \in N})$. This gated architecture allows each agent to maintain a compact yet informative representation of their observation history, enabling them to learn sophisticated social learning strategies while keeping the state space tractable.

\subsection*{Centralized Training with Decentralized Execution}
We adopt a CTDE paradigm where each agent maintains a decentralized actor network $\pi^i_{\phi_i}$ for action execution, while training utilizes a centralized critic network $Q_{\xi}$ that has access to the global state. This approach preserves the decentralized nature of action execution while leveraging additional information during training, which mimics the role of a \textit{social planner.}

\subsubsection*{Decentralized Actor Network}
The decentralized actor network for agent $i$ maps the hidden state to a policy distribution as before,
\[
    \pi^i_{\phi_i}(a^i_t|h^i_t) = \text{softmax}(W^i_{\pi}h^i_t + b^i_{\pi}).
\]
Then, we use the policy gradient update to optimize this network given as,
\[
    \nabla_{\phi_i} J(\phi_i) = \mathbb{E}\left[\nabla_{\phi_i} \log \pi^i_{\phi_i}(a^i_t|h^i_t) A^i(\tilde{\psi}_t)\right]
\]
where $A^i(\tilde{\psi}_t)$ is the advantage function computed using the centralized critic.

\subsubsection*{Centralized Critic Network}

The centralized critic takes as input the global state $\psi_t$ and all agents' hidden states:
\[
    Q_{\xi}(\psi_t, \mathbf{h}_t) = W_Q[\psi_t, \mathbf{h}_t, \mathbf{a}_t] + b_Q
\]
where $\mathbf{h}_t = (h^1_t, \ldots, h^n_t)$. The critic network is updated to minimize the \textit{temporal difference} error with the loss function,
\[
    \mathcal{L}(\xi) = \mathbb{E}\left[(r_t + \gamma Q_{\xi}(\tilde{\psi}_{t+1}) - Q_{\xi}(\tilde{\psi}_t))^2\right].
\]
The advantage function for each agent is computed as,
\[
    A^i(\tilde{\psi}_t) = Q_{\xi}(\tilde{\psi}_t) - V_{\xi}(\tilde{\psi}_t),
\]
where $V_{\xi}(\tilde{\psi}_t) = \mathbb{E}_{\mathbf{a}_t \sim \boldsymbol{\pi}}[Q_{\xi}(\tilde{\psi}_t]$ is the value function.


\subsubsection*{Learning Algorithm}
The complete learning process alternates between the actor network update,
\[
    \phi_i \leftarrow \phi_i + \alpha \nabla_{\phi_i} J(\phi_i),
\]
and the critic network update,
\[
    \xi \leftarrow \xi - \beta \nabla_{\xi} \mathcal{L}(\xi).
\]









\pagebreak

\section*{Appendix A: Theoretical Background}

\subsection*{Deep Reinforcement Learning Framework}

The foundation of our deep reinforcement learning approach builds upon the policy gradient theorem \citep{sutton2000policy} and its extensions to deep neural networks \citep{mnih2015human}. In the context of multi-agent systems, we specifically utilize the actor-critic framework \citep{konda2000actor}, which has been shown to be particularly effective in handling the increased complexity of multi-agent environments \citep{lowe2017multi}.

\subsection*{Policy Gradient Methods}

The core objective in reinforcement learning is to maximize the expected cumulative discounted reward,
\[
J(\phi) = \mathbb{E}_{\tau \sim \pi_\phi}[\sum_{t=0}^{\infty} \gamma^t r_t]
\]
where $\tau$ represents a trajectory, and $\pi_\phi$ is the policy parameterized by $\phi$. The policy gradient theorem \citep{sutton2000policy} provides the gradient of this objective,
\[
\nabla_\phi J(\phi) = \mathbb{E}_{\tau \sim \pi_\phi}[\sum_{t=0}^{\infty} \nabla_\phi \log \pi_\phi(a_t|s_t) Q^{\pi_\phi}(s_t, a_t)]
\]
In practice, we use the advantage function $A(s_t, a_t)$ instead of the Q-function to reduce variance \citep{schulman2015high}:

\[
\nabla_\phi J(\phi) = \mathbb{E}_{\tau \sim \pi_\phi}[\sum_{t=0}^{\infty} \nabla_\phi \log \pi_\phi(a_t|s_t) A(s_t, a_t)]
\]

\subsection*{Value Function Approximation}
The value function approximation follows the temporal difference (TD) learning framework \citep{sutton2018reinforcement}. The value network parameters $\phi$ are updated to minimize the mean squared TD error:

\[
\mathcal{L}(\phi) = \mathbb{E}_{(s_t, a_t, r_t, s_{t+1}) \sim \mathcal{D}}[(r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t))^2]
\]

where $\mathcal{D}$ represents the replay buffer of experiences. This approach has been shown to be particularly effective when combined with deep neural networks \citep{mnih2015human}.

\subsubsection*{LSTM Implementation}

In practice, we use Long Short-Term Memory (LSTM) cells to better capture long-term dependencies in the information sets. The LSTM update equations for each agent $i$ are:
\begin{align}
    f^i_t &= \sigma(W^i_f [h^i_{t-1}, s^i_t, \mathbf{a}_t] + b^i_f) \\
    \iota^i_t &= \sigma(W^i_\iota [h^i_{t-1}, s^i_t, \mathbf{a}_t] + b^i_\iota) \\
    \tilde{c}^i_t &= \tanh(W^i_c [h^i_{t-1}, s^i_t, \mathbf{a}_t] + b^i_c) \\
    c^i_t &= f^i_t \odot c^i_{t-1} + \iota^i_t \odot \tilde{c}^i_t \\
    o^i_t &= \sigma(W^i_o [h^i_{t-1}, s^i_t, \mathbf{a}_t] + b^i_o) \\
    h^i_t &= o^i_t \odot \tanh(c^i_t)
\end{align}
where $c^i_t$ is the cell state that allows for longer-term memory preservation.

\subsection*{Attention-Enhanced RNN Architecture}

We extend the base RNN architecture by incorporating both self-attention and cross-attention mechanisms to better capture temporal dependencies and agent interactions. The attention mechanisms allow each agent to selectively focus on relevant historical information and other agents' actions.

\subsubsection*{Self-Attention Mechanism}
For each agent $i$, we implement a self-attention mechanism that operates over a window of $K$ previous hidden states. Let $H_t^i = [h_{t-K+1}^i, ..., h_t^i] \in \mathbb{R}^{d \times K}$ be the matrix of $K$ previous hidden states. The self-attention operation is defined as:

\begin{align*}
Q_t^i &= W_Q^i h_t^i + b_Q^i & \text{(query)} \\
K_t^i &= W_K^i H_t^i + b_K^i & \text{(keys)} \\
V_t^i &= W_V^i H_t^i + b_V^i & \text{(values)} \\
\alpha_t^i &= \text{softmax}\left(\frac{(Q_t^i)^\top K_t^i}{\sqrt{d}}\right) & \text{(attention weights)} \\
c_t^i &= V_t^i \alpha_t^i & \text{(context vector)}
\end{align*}

where $W_Q^i, W_K^i, W_V^i$ are learned weight matrices and $b_Q^i, b_K^i, b_V^i$ are bias vectors. The scaling factor $\sqrt{d}$ prevents the dot products from growing too large in magnitude.

\subsubsection*{Cross-Attention for Agent Interactions}
To better capture agent interactions, we implement a cross-attention mechanism that allows each agent to attend to other agents' hidden states. For agent $i$, let $H_t^{-i} = [h_t^1, ..., h_t^{i-1}, h_t^{i+1}, ..., h_t^n] \in \mathbb{R}^{d \times (n-1)}$ be the matrix of other agents' current hidden states. The cross-attention operation is:

\begin{align*}
Q_t^{i,c} &= W_{Q,c}^i h_t^i + b_{Q,c}^i & \text{(cross query)} \\
K_t^{i,c} &= W_{K,c}^i H_t^{-i} + b_{K,c}^i & \text{(cross keys)} \\
V_t^{i,c} &= W_{V,c}^i H_t^{-i} + b_{V,c}^i & \text{(cross values)} \\
\beta_t^i &= \text{softmax}\left(\frac{(Q_t^{i,c})^\top K_t^{i,c}}{\sqrt{d}}\right) & \text{(cross-attention weights)} \\
m_t^i &= V_t^{i,c} \beta_t^i & \text{(cross-context vector)}
\end{align*}

\subsubsection*{Enhanced GRU Architecture}
We modify the original GRU architecture to incorporate both attention mechanisms. The updated equations become:

\begin{align*}
\hat{h}_t^i &= [h_t^i; c_t^i; m_t^i] & \text{(concatenated state)} \\
z_t^i &= \text{sigmoid}(W_z^i[\hat{h}_t^i, s_t^i, \mathbf{a}_t] + b_z^i) & \text{(update gate)} \\
\rho_t^i &= \text{sigmoid}(W_\rho^i[\hat{h}_t^i, s_t^i, \mathbf{a}_t] + b_\rho^i) & \text{(reset gate)} \\
\tilde{h}_t^i &= \tanh(W_h^i[\rho_t^i \odot \hat{h}_t^i, s_t^i, \mathbf{a}_t] + b_h^i) & \text{(candidate state)} \\
h_t^i &= (1-z_t^i) \odot \hat{h}_t^i + z_t^i \odot \tilde{h}_t^i & \text{(final update)}
\end{align*}

\subsubsection*{Multi-Head Attention Extension}
To capture different types of dependencies simultaneously, we extend both attention mechanisms to use multiple attention heads. For $L$ attention heads, each head $l \in \{1, ..., L\}$ computes its own attention weights and context vectors:

\begin{align*}
Q_{t,l}^i &= W_{Q,l}^i h_t^i + b_{Q,l}^i \\
K_{t,l}^i &= W_{K,l}^i H_t^i + b_{K,l}^i \\
V_{t,l}^i &= W_{V,l}^i H_t^i + b_{V,l}^i \\
\alpha_{t,l}^i &= \text{softmax}\left(\frac{(Q_{t,l}^i)^\top K_{t,l}^i}{\sqrt{d/L}}\right) \\
c_{t,l}^i &= V_{t,l}^i \alpha_{t,l}^i
\end{align*}

The final context vector is computed by concatenating the outputs from all heads and applying a linear transformation:

\[
c_t^i = W_O^i[c_{t,1}^i; ...; c_{t,L}^i] + b_O^i
\]

Similarly for the cross-attention mechanism:

\begin{align*}
Q_{t,l}^{i,c} &= W_{Q,c,l}^i h_t^i + b_{Q,c,l}^i \\
K_{t,l}^{i,c} &= W_{K,c,l}^i H_t^{-i} + b_{K,c,l}^i \\
V_{t,l}^{i,c} &= W_{V,c,l}^i H_t^{-i} + b_{V,c,l}^i \\
\beta_{t,l}^i &= \text{softmax}\left(\frac{(Q_{t,l}^{i,c})^\top K_{t,l}^{i,c}}{\sqrt{d/L}}\right) \\
m_{t,l}^i &= V_{t,l}^{i,c} \beta_{t,l}^i \\
m_t^i &= W_{O,c}^i[m_{t,1}^i; ...; m_{t,L}^i] + b_{O,c}^i
\end{align*}

\subsubsection*{Learning Considerations}
The addition of attention mechanisms introduces several hyperparameters that need to be tuned:
\begin{itemize}
\item History window size $K$ for self-attention
\item Number of attention heads $L$
\item Hidden dimension $d$ for attention computations
\end{itemize}

The training process remains similar to the original architecture, but with additional parameters in both actor and critic networks. The critic network now also incorporates attention mechanisms when processing the global state:

\[
Q_{\xi}(\psi_t, \mathbf{h}_t) = W_Q[\psi_t, \text{Attention}(\mathbf{h}_t), \mathbf{a}_t] + b_Q
\]

where $\text{Attention}(\mathbf{h}_t)$ applies both self and cross-attention mechanisms to the collection of all agents' hidden states.

\section*{Detailed Multi-Agent Attention PPO (MA-APPO) Algorithm}

\subsection*{Network Architectures}

\subsubsection*{1. Actor Network Architecture}

For each agent $i$, the actor network consists of:

\begin{enumerate}
\item \textbf{Input Processing Layer:}
   \[
   x_t^i = W_{\text{in}}[s_t^i; \mathbf{a}_t] + b_{\text{in}}
   \]

\item \textbf{GRU Layer with Attention:}
   \begin{align*}
   z_t^i &= \sigma(W_z^i[h_{t-1}^i, x_t^i] + b_z^i) \\
   r_t^i &= \sigma(W_r^i[h_{t-1}^i, x_t^i] + b_r^i) \\
   \tilde{h}_t^i &= \tanh(W_h^i[r_t^i \odot h_{t-1}^i, x_t^i] + b_h^i) \\
   h_t^i &= (1-z_t^i) \odot h_{t-1}^i + z_t^i \odot \tilde{h}_t^i
   \end{align*}

\item \textbf{Self-Attention Module:}
   \begin{align*}
   Q_t^i &= W_Q^i h_t^i + b_Q^i \\
   K_t^i &= W_K^i H_t^i + b_K^i \\
   V_t^i &= W_V^i H_t^i + b_V^i \\
   \alpha_t^i &= \text{softmax}\left(\frac{Q_t^i(K_t^i)^\top}{\sqrt{d_k}}\right) \\
   c_t^i &= V_t^i\alpha_t^i
   \end{align*}

\item \textbf{Cross-Attention Module:}
   \begin{align*}
   Q_t^{i,c} &= W_{Q,c}^i h_t^i + b_{Q,c}^i \\
   K_t^{i,c} &= W_{K,c}^i H_t^{-i} + b_{K,c}^i \\
   V_t^{i,c} &= W_{V,c}^i H_t^{-i} + b_{V,c}^i \\
   \beta_t^i &= \text{softmax}\left(\frac{Q_t^{i,c}(K_t^{i,c})^\top}{\sqrt{d_k}}\right) \\
   m_t^i &= V_t^{i,c}\beta_t^i
   \end{align*}

\item \textbf{Policy Head:}
   \[
   \pi_{\phi_i}(a|h_t^i) = \text{softmax}(W_\pi[h_t^i; c_t^i; m_t^i] + b_\pi)
   \]
\end{enumerate}

\subsubsection*{2. Critic Network Architecture}

The centralized critic network processes global state information:

\begin{enumerate}
\item \textbf{State Embedding:}
   \[
   e_t = W_e[\psi_t; \mathbf{h}_t] + b_e
   \]

\item \textbf{Multi-head Attention:}
   For each head $l \in \{1,\ldots,L\}$:
   \begin{align*}
   Q_{t,l} &= W_{Q,l}e_t + b_{Q,l} \\
   K_{t,l} &= W_{K,l}e_t + b_{K,l} \\
   V_{t,l} &= W_{V,l}e_t + b_{V,l} \\
   \text{head}_l &= \text{softmax}\left(\frac{Q_{t,l}K_{t,l}^\top}{\sqrt{d_k}}\right)V_{t,l}
   \end{align*}

\item \textbf{Value Head:}
   \[
   V(s_t) = W_v[\text{concat}(\text{head}_1,\ldots,\text{head}_L)] + b_v
   \]
\end{enumerate}

\subsection*{Detailed Training Algorithm}

\begin{enumerate}
\item \textbf{Initialization:}
   \begin{itemize}
   \item Initialize actor networks $\pi_{\phi_i}$ for each agent $i$
   \item Initialize centralized critic network $V_\theta$
   \item Set up experience buffer $\mathcal{D}$ with capacity $N$
   \item Initialize environment runners for parallel data collection
   \end{itemize}

\item \textbf{Main Training Loop:}
   \begin{enumerate}
   \item \textbf{Data Collection Phase:}
      \begin{itemize}
      \item Run $M$ parallel environments for $T$ timesteps
      \item For each timestep $t$ and agent $i$:
         \begin{itemize}
         \item Process observation $o_t^i$ through GRU and attention layers
         \item Sample action $a_t^i \sim \pi_{\phi_i}(\cdot|h_t^i)$
         \item Store tuple $(o_t^i, a_t^i, r_t^i, h_t^i)$ in buffer $\mathcal{D}$
         \end{itemize}
      \end{itemize}

   \item \textbf{Advantage Estimation:}
      \begin{itemize}
      \item Compute returns using GAE-$\lambda$:
      \[
      \hat{R}_t = r_t + \gamma\lambda r_{t+1} + (\gamma\lambda)^2r_{t+2} + \cdots
      \]
      \item Calculate advantages:
      \[
      \hat{A}_t = \hat{R}_t - V_\theta(s_t)
      \]
      \item Normalize advantages:
      \[
      \hat{A}_t \leftarrow \frac{\hat{A}_t - \mu_{\hat{A}}}{\sigma_{\hat{A}} + \epsilon}
      \]
      \end{itemize}

   \item \textbf{Policy Update Phase:}
      \begin{itemize}
      \item For $K$ epochs:
         \begin{itemize}
         \item Sample mini-batches of size $B$ from $\mathcal{D}$
         \item Compute policy loss:
         \[
         L_{\text{CLIP}}(\phi_i) = \mathbb{E}_t[\min(r_t(\phi_i)\hat{A}_t, \text{clip}(r_t(\phi_i), 1-\epsilon, 1+\epsilon)\hat{A}_t)]
         \]
         \item Compute value function loss:
         \[
         L_{\text{VF}}(\theta) = \frac{1}{2}\mathbb{E}_t[(V_\theta(s_t) - \hat{R}_t)^2]
         \]
         \item Compute attention losses:
         \begin{align*}
         L_{\text{self}} &= -\sum_t\sum_{k=1}^K \alpha_t^i(k)\log(\alpha_t^i(k)) \\
         L_{\text{cross}} &= -\sum_t\sum_{j\neq i} \beta_t^i(j)\log(\beta_t^i(j))
         \end{align*}
         \item Update parameters:
         \begin{align*}
         \phi_i &\leftarrow \phi_i + \eta_\pi\nabla_{\phi_i}(L_{\text{CLIP}} + c_1L_{\text{self}} + c_2L_{\text{cross}}) \\
         \theta &\leftarrow \theta - \eta_v\nabla_\theta L_{\text{VF}}
         \end{align*}
         \end{itemize}
      \end{itemize}
   \end{enumerate}
\end{enumerate}

\subsection*{Detailed Hyperparameter Settings}

\begin{enumerate}
\item \textbf{Network Architecture:}
   \begin{itemize}
   \item Input embedding dimension: 128
   \item GRU hidden dimension: 256
   \item Attention key/value dimension: 64
   \item Number of attention heads: 8
   \item Feed-forward hidden dimension: 512
   \item Layer normalization epsilon: $1 \times 10^{-5}$
   \end{itemize}

\item \textbf{Training Parameters:}
   \begin{itemize}
   \item Learning rate (actor): $3 \times 10^{-4}$
   \item Learning rate (critic): $1 \times 10^{-3}$
   \item Batch size: 64
   \item Number of epochs: 10
   \item Clipping parameter $\epsilon$: 0.2
   \item GAE parameter $\lambda$: 0.95
   \item Discount factor $\gamma$: 0.99
   \item Gradient clipping norm: 0.5
   \end{itemize}

\item \textbf{Attention-Specific:}
   \begin{itemize}
   \item History window size: 50
   \item Attention dropout rate: 0.1
   \item Temperature for attention softmax: 1.0
   \item Self-attention weight $c_1$: 0.1
   \item Cross-attention weight $c_2$: 0.1
   \end{itemize}

\item \textbf{Buffer Parameters:}
   \begin{itemize}
   \item Buffer capacity: $1 \times 10^6$
   \item Minimum buffer size for training: $1 \times 10^4$
   \item Number of parallel environments: 16
   \item Steps per environment: 128
   \end{itemize}
\end{enumerate}

\subsection*{Training Optimizations}

\begin{enumerate}
\item \textbf{Memory Management:}
   \begin{itemize}
   \item Use half-precision (FP16) training
   \item Implement gradient checkpointing
   \item Employ efficient attention computation using sparse attention
   \item Implement circular buffer for history management
   \end{itemize}

\item \textbf{Computational Optimizations:}
   \begin{itemize}
   \item Vectorize environment interactions
   \item Use GPU-accelerated tensor operations
   \item Implement parallel advantage computation
   \item Use efficient mini-batch sampling
   \end{itemize}

\item \textbf{Stability Techniques:}
   \begin{itemize}
   \item Gradient norm clipping
   \item Learning rate scheduling
   \item Value function clipping
   \item Entropy regularization
   \item Early stopping based on KL divergence
   \end{itemize}

\item \textbf{Curriculum Learning:}
   \begin{itemize}
   \item Progressive attention head annealing
   \item Gradual increase in history window size
   \item Adaptive batch size scheduling
   \item Dynamic learning rate adjustment
   \end{itemize}
\end{enumerate}